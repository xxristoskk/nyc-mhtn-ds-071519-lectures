{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Off?\n",
    "\n",
    "You are a data scientist for the MTA. For your first project, they want you to predict the number of subway riders for each day. You decide to do a linear regression model predict the riders but need to gather data first. With a partner brainstorm a list of different variables you think would explain the number of daily riders.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "Agenda Today:\n",
    "\n",
    "- Create a model for multiple linear regression\n",
    "- Interpret the output for multiple linear regression\n",
    "- Multicollinearity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple linear regression in python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "sns.set(style=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in car data\n",
    "df = sns.load_dataset('mpg')\n",
    "#read in movie data\n",
    "movie_df = pd.read_csv('cleaned_movie_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a linear regression model using statsmodel \n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "lr_model = ols(formula='mpg~weight', data=df).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe what you think the following things are doing.\n",
    "\n",
    "`ols()`\n",
    "\n",
    "`formula = 'mpg~weight`\n",
    "\n",
    "`data=df`\n",
    "\n",
    "`fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "Multiple linear regression is simply a linear regression with more than one predictor, or independent variables. Let's recall the interpretation of $R^2$ in simple linear regression represents the proportion of variance explained by the model. What if we make the model more complex by including more predictors in it such that it account for even more variance in the outcome?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + + \\beta_3 X_3\\cdots + \\beta_k X_k + \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "sen = np.random.uniform(18, 65, 100)\n",
    "income = np.random.normal((sen/10), 0.5)\n",
    "sen = sen.reshape(-1,1)\n",
    "\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "fig.suptitle('seniority vs. income', fontsize=16)\n",
    "plt.scatter(sen, income)\n",
    "plt.plot(sen, sen/10, c = \"black\")\n",
    "plt.xlabel(\"seniority\", fontsize=14)\n",
    "plt.ylabel(\"monthly income\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we include another factor, such as years of education? All that is doing is adding a higher dimensional object to the model, so our model will be three dimensional. \n",
    "<img src=\"multi_reg_graph.png\" style=\"withd:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_model = ols(formula='mpg~weight+horsepower+displacement+cylinders+acceleration', data=df).fit()\n",
    "mlr_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of the Model Parameters\n",
    "- Each β parameter represents the change in the mean response, E(y), per unit increase in the associated predictor variable when all the other predictors are held constant.\n",
    "- For example, β1 represents the estimated change in the mean response, E(y), per unit increase in x1 when x2, x3, ..., xp−1 are held constant.\n",
    "- The intercept term, β0, represents the estimated mean response, E(y), when all the predictors x1, x2, ..., xp−1, are all zero (which may or may not have any practical meaning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "\n",
    "Create a using the code example above create a multiple linear regression model to predict the gross revenue of movies.  \n",
    "\n",
    "***For now, do not put in any categorical variables like movie rating***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is the Adjusted R-squared?\n",
    "\n",
    "The adjusted R-squared compares the explanatory power of regression models that contain different numbers of predictors.\n",
    "\n",
    "Suppose you compare a five-predictor model with a higher R-squared to a one-predictor model. Does the five predictor model have a higher R-squared because it’s better? Or is the R-squared higher because it has more predictors? Simply compare the adjusted R-squared values to find out!\n",
    "\n",
    "$$Adjusted R^2=1-\\left(\\frac{n-1}{n-p}\\right)(1-R^2)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "n = sample size   \n",
    "\n",
    "p  = the number of independent variables in the regression equation\n",
    "\n",
    "\n",
    "- The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. \n",
    "\n",
    "- The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. \n",
    "\n",
    "- It is always lower than the R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity \n",
    "\n",
    "**Multicollinearity** occurs when independent variables in a regression model are very highly correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results.\n",
    "\n",
    "The interpretation of a regression coefficient is that it represents the mean change in the dependent variable for each 1 unit change in an independent variable when you hold all of the other independent variables constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two basic kinds of multicollinearity:\n",
    "\n",
    "- **Structural multicollinearity:** This type occurs when we create a model term using other terms. In other words, it’s a byproduct of the model that we specify rather than being present in the data itself. For example, if you square term X to model curvature, clearly there is a correlation between X and X2.\n",
    "- **Data multicollinearity:** This type of multicollinearity is present in the data itself rather than being an artifact of our model. Observational experiments are more likely to exhibit this kind of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Problems Do Multicollinearity Cause?\n",
    "\n",
    "Multicollinearity causes the following two basic types of problems:\n",
    "\n",
    "- The coefficient estimates can swing wildly based on which other independent variables are in the model. The coefficients become very sensitive to small changes in the model.\n",
    "- Multicollinearity reduces the precision of the estimate coefficients, which weakens the statistical power of your regression model. You might not be able to trust the p-values to identify independent variables that are statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do I Have to Fix Multicollinearity?\n",
    "\n",
    "The need to reduce multicollinearity depends on its severity and your primary goal for your regression model. Keep the following three points in mind:\n",
    "\n",
    "- The severity of the problems increases with the degree of the multicollinearity. Therefore, if you have only moderate multicollinearity, you may not need to resolve it.\n",
    "- Multicollinearity affects only the specific independent variables that are correlated. Therefore, if multicollinearity is not present for the independent variables that you are particularly interested in, you may not need to resolve it. \n",
    "- Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you don’t need to understand the role of each independent variable, you don’t need to reduce severe multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***That being said, the easies way to deal with multicollinearity is just to remove one of the variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(df.corr())\n",
    "plt.xticks(range(len(df.columns)), df.columns)\n",
    "plt.yticks(range(len(df.columns)), df.columns)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Better Looking Heatmap with Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CorrMtx(df, dropDuplicates = True):\n",
    "\n",
    "    # Your dataset is already a correlation matrix.\n",
    "    # If you have a dateset where you need to include the calculation\n",
    "    # of a correlation matrix, just uncomment the line below:\n",
    "    # df = df.corr()\n",
    "\n",
    "    # Exclude duplicate correlations by masking uper right values\n",
    "    if dropDuplicates:    \n",
    "        mask = np.zeros_like(df, dtype=np.bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set background color / chart style\n",
    "    sns.set_style(style = 'white')\n",
    "\n",
    "    # Set up  matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Add diverging colormap from red to blue\n",
    "    cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "\n",
    "    # Draw correlation plot with or without duplicates\n",
    "    if dropDuplicates:\n",
    "        sns.heatmap(df, mask=mask, cmap=cmap, \n",
    "                square=True,\n",
    "                linewidth=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "    else:\n",
    "        sns.heatmap(df, cmap=cmap, \n",
    "                square=True,\n",
    "                linewidth=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "\n",
    "\n",
    "CorrMtx(corr, dropDuplicates = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more examples to make your correlation heatmap look good\n",
    "https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun the Model After Removing the highly correlate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_model = ols(formula='mpg~weight+horsepower+cylinders+acceleration', data=df).fit()\n",
    "mlr_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Movie Data\n",
    "\n",
    "Identify any variables with a high correlation, remove one of them, and rerun your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## yoru code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Everything about regression:  https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-tutorial-and-examples\n",
    "\n",
    "Statsmodels example: https://datatofish.com/statsmodels-linear-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
