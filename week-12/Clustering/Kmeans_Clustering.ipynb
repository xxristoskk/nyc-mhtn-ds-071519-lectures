{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](http://drive.google.com/uc?export=view&id=1Gi4lNIANsWjxlvikM4h1Xein_PPwhHgM)\n",
    "# **Introduction to K-means Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Off:\n",
    "\n",
    "What is the difference between supervised learning and unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda:\n",
    "- Unsupervised Learning\n",
    "- K-means Concepts\n",
    "- Selecting the Right number of K\n",
    "- Problems with K-Means\n",
    "- K-Means Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Vs Unsupervised Learning.\n",
    "In supervised learning, the system tries to learn from the previous examples that are given. (On the other hand, in unsupervised learning, the system attempts to find the patterns directly from the example given.) So if the dataset is labelled it comes under a supervised problem, it the dataset is unlabelled then it is an unsupervised problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://cdn-images-1.medium.com/max/800/1*AZMDyaifxGVdwTV-1BN7kA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "In clustering, the data is divided into several groups. In plain words, the aim is to segregate groups with similar traits and assign them into clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![clustering](https://cdn-images-1.medium.com/max/800/1*58tBPk4oZqhZ-LUq-0Huow.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Use Cases:\n",
    "\n",
    "Talk with a partner and come up with two problems wher you think clustering could be of some use.\n",
    "    \n",
    "https://dzone.com/articles/10-interesting-use-cases-for-the-k-means-algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering: Types\n",
    "Clustering can be broadly divided into two subgroups:\n",
    "\n",
    "**Hard clustering**: in hard clustering, each data object or point either belongs to a cluster completely or not. For example in the Uber dataset, each location belongs to either one borough or the other.\n",
    "\n",
    "**Soft clustering**: in soft clustering, a data point can belong to more than one cluster with some probability or likelihood value. For example, you could identify some locations as the border points belonging to two or more boroughs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Algorithms\n",
    "\n",
    "**Connectivity-based clustering**: the main idea behind this clustering is that data points that are closer in the data space are more related (similar) than to data points farther away. The clusters are formed by connecting data points according to their distance. At different distances, different clusters will form and can be represented using a dendrogram, which gives away why they are also commonly called \"hierarchical clustering\". These methods do not produce a unique partitioning of the dataset, rather a hierarchy from which the user still needs to choose appropriate clusters by choosing the level where they want to cluster. They are also not very robust towards outliers, which might show up as additional clusters or even cause other clusters to merge.\n",
    "\n",
    "**Centroid-based clustering**: in this type of clustering, clusters are represented by a central vector or a centroid. This centroid might not necessarily be a member of the dataset. This is an iterative clustering algorithms in which the notion of similarity is derived by how close a data point is to the centroid of the cluster. k-means is a centroid based clustering, and will you see this topic more in detail later on in the tutorial.\n",
    "\n",
    "**Distribution-based clustering**: this clustering is very closely related to statistics: distributional modeling. Clustering is based on the notion of how probable is it for a data point to belong to a certain distribution, such as the Gaussian distribution, for example. Data points in a cluster belong to the same distribution. These models have a strong theoritical foundation, however they often suffer from overfitting. Gaussian mixture models, using the expectation-maximization algorithm is a famous distribution based clustering method.\n",
    "\n",
    "**Density-based** methods search the data space for areas of varied density of data points. Clusters are defined as areas of higher density within the data space compared to other regions. Data points in the sparse areas are usually considered to be noise and/or border points. The drawback with these methods is that they expect some kind of density guide or parameters to detect cluster borders. DBSCAN and OPTICS are some prominent density based clustering.\n",
    "\n",
    "\n",
    "https://medium.com/predict/three-popular-clustering-methods-and-when-to-use-each-4227c80ba2b6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Unsupervised Techniques:\n",
    "- Principal Component Analysis (PCA)\n",
    "- Anomaly detection\n",
    "- Autoencoders\n",
    "- Deep Belief Nets\n",
    "- Hebbian Learning\n",
    "- Generative Adversarial Networks(GANs)\n",
    "- Self-Organizing maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries and download dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Required packages for today\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "\n",
    "# Familiar packages for plotting, data manipulation, and numeric functions\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Have plots appear in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Default plot params\n",
    "plt.style.use('seaborn')\n",
    "cmap = 'tab10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data for later in the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/mubaris/friendly-fortnight/master/xclara.csv'\n",
    "test = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering!   Finding **GROUPS**\n",
    "\n",
    "How many groups do you see?\n",
    "\n",
    "![img](http://drive.google.com/uc?export=view&id=1am-0t4grAQueYP9IMmgV0dC4MHWTKd6h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  K-Means approach\n",
    "\n",
    "Takes K number of centroids or data points in its account initially.\n",
    "After choosing the centroids, (say C1 and C2) the data points (coordinates here) are assigned to any of the Clusters (let’s take centroids = clusters for the time being) depending upon the distance between them and the centroids.\n",
    "\n",
    "For measuring the distances, you take the following distance measurement function (also termed as similarity measurement function):\n",
    "\n",
    "$$d = |x2 – x1| + | y2 – y1| + |z2 – z1|$$\n",
    "\n",
    "This is also known as the Taxicab distance or Manhattan distance, where d is distance measurement between two objects, (x1,y1,z1) and (x2,y2,z2) are the X, Y and Z coordinates of any two objects taken for distance measurement.\n",
    "\n",
    "Feel free to check out other distance measurement functions like [Euclidean Distance, Cosine Distance etc](https://arxiv.org/ftp/arxiv/papers/1405/1405.7471.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm will continue updating cluster centroids (i.e the coordinates) until they cannot be updated anymore (more on when it cannot be updated later). The update takes place in the following manner:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![centroid_update](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526380253/CodeCogsEqn_v5le8c.png)\n",
    "\n",
    "(where n = number of objects belonging to that particular cluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm no longer updates the centroids when there is no change in the current cluster formation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe the following four examples with a sample dataset:\n",
    "\n",
    "#### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![left](http://drive.google.com/uc?export=view&id=1rQsgwmbeKsf9lgAv4onHJQMNqYcXZ1cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![right](http://drive.google.com/uc?export=view&id=1duSB73v8ugD5liZyn5dUeSOgwvAbdgui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![top](http://drive.google.com/uc?export=view&id=1ZRa7oAskuwSQxLqEyYQc8FZWjERnQx_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![bottom](http://drive.google.com/uc?export=view&id=1TE62xmACOaux75AlzivcP7tSzR86n8ZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Questions:\n",
    "\n",
    "- What do they have in common?\n",
    "- What are the differences between them?\n",
    "- How many groups are there in the end?\n",
    "- Do you see any problems with this method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Data\n",
    "\n",
    "We will now use the dataset we uploaded earlier:\n",
    "\n",
    "`url = 'https://raw.githubusercontent.com/mubaris/friendly-fortnight/master/xclara.csv'`\n",
    "\n",
    "`test = pd.read_csv(url)`\n",
    "\n",
    "- This is a sample dataset. \n",
    "- Let us assume the data is already scaled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction of `Kmeans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=4).fit(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note! \n",
    "#### Do you have different cluster_centers?\n",
    "*Good!*\n",
    "\n",
    "We saw in the demo that the algorithm is sensitive to starting points.\n",
    "\n",
    "We can use the additional argument `random_state` to set the seed and have a repeatable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_setseed = KMeans(n_clusters=4, random_state=10).fit(test)\n",
    "model_setseed.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise**: \n",
    "- Try running `Kmeans` with different number of `n_clusters`, k=2 through k=7\n",
    "- Check the `cluster_centers_` \n",
    "- Without running any more functions, which number of K is the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practice code goes here\n",
    "model5 = KMeans(n_clusters=5, random_state=8).fit(test)\n",
    "model5.cluster_centers_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Determining Optimal Number of Clusters\n",
    "\n",
    "\n",
    "A variety of measures have been proposed in the literature for evaluating clustering results. The term clustering validation is used to design the procedure of evaluating the results of a clustering algorithm. There are more than thirty indices and methods for identifying the optimal number of clusters so I’ll just focus on a two. \n",
    "\n",
    "#### Two metrics we can use: **elbow method** and the **silhouette coefficient**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Elbow Method\n",
    "Recall that, the basic idea behind partitioning methods, such as k-means clustering, is to define clusters such that the total intra-cluster variation [or total within-cluster sum of square (WSS)] is minimized. The total WSS measures the compactness of the clustering and we want it to be as small as possible.\n",
    "\n",
    "The Elbow method looks at the total WSS as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn’t improve much better the total WSS.\n",
    "\n",
    "The optimal number of clusters can be defined as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the dataset and initializing variables\n",
    "X = test\n",
    "distorsions = []\n",
    "\n",
    "# Calculate SSE for different K\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k, random_state = 301)\n",
    "    kmeans.fit(X)\n",
    "    distorsions.append(kmeans.inertia_)\n",
    "\n",
    "# Plot values of SSE\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(121, title='Elbow curve')\n",
    "plt.xlabel('k')\n",
    "plt.plot(range(2, 10), distorsions)\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(test.V1, test.V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette coefficient\n",
    "\n",
    "The average silhouette approach we’ll be described comprehensively in the chapter [cluster validation statistics](https://www.datanovia.com/en/lessons/cluster-validation-statistics-must-know-methods/).\n",
    "\n",
    "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.\n",
    "\n",
    "The silhouette can be calculated with any distance metric, such as the Euclidean distance or the Manhattan distance.\n",
    "\n",
    "Average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximize the average silhouette over a range of possible values for k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate silhouette coefficient for each k\n",
    "X = test\n",
    "silhouette_plot = []\n",
    "for k in range(2, 10):\n",
    "    clusters = KMeans(n_clusters=k, random_state=10)\n",
    "    cluster_labels = clusters.fit_predict(X)\n",
    "    silhouette_avg = metrics.silhouette_score(X, cluster_labels)\n",
    "    silhouette_plot.append(silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Silhouette coefficient\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(121, title='Silhouette coefficients over k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('silhouette coefficient')\n",
    "plt.plot(range(2, 10), silhouette_plot)\n",
    "plt.axhline(y=np.mean(silhouette_plot), color=\"red\", linestyle=\"--\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Assumptions** and **challenges** of K-means\n",
    "\n",
    "http://varianceexplained.org/r/kmeans-free-lunch/\n",
    "\n",
    "- Demonstrate the ideal K-means dataset\n",
    "- Show three scenarios where K-means struggles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messy Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "\n",
    "#Toy data sets\n",
    "centers_neat = [(-10, 10), (0, -5), (10, 5)]\n",
    "x_neat, _ = datasets.make_blobs(n_samples=5000,\n",
    "                                centers=centers_neat,\n",
    "                                cluster_std=2,\n",
    "                                random_state=2)\n",
    "\n",
    "x_messy, _ = datasets.make_classification(n_samples=5000,\n",
    "                                          n_features=10,\n",
    "                                          n_classes=3,\n",
    "                                          n_clusters_per_class=1,\n",
    "                                          class_sep=1.5,\n",
    "                                          shuffle=False,\n",
    "                                          random_state=301)\n",
    "#Default plot params\n",
    "plt.style.use('seaborn')\n",
    "cmap = 'tab10'\n",
    "\n",
    "plt.figure(figsize=(17,8))\n",
    "plt.subplot(121, title='\"Neat\" Clusters')\n",
    "plt.scatter(x_neat[:,0], x_neat[:,1])\n",
    "plt.subplot(122, title='\"Messy\" Clusters')\n",
    "plt.scatter(x_messy[:,0], x_messy[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Predict K-Means cluster membership\n",
    "km_neat = KMeans(n_clusters=3, random_state=2).fit_predict(x_neat)\n",
    "km_messy = KMeans(n_clusters=3, random_state=2).fit_predict(x_messy)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(121, title='\"Neat\" K-Means')\n",
    "plt.scatter(x_neat[:,0], x_neat[:,1], c=km_neat, cmap=cmap)\n",
    "plt.subplot(122, title='\"Messy\" K-Means')\n",
    "plt.scatter(x_messy[:,0], x_messy[:,1], c=km_messy, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Spherical Data\n",
    "\n",
    "How do you think the k-means algorithm will handle non-spherical clusters like these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nonspherical](http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/non_spherical-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe this isn’t what you were expecting- but it’s a perfectly reasonable way to construct clusters. Looking at this image, we humans immediately recognize two natural groups of points- there’s no mistaking them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![clusteringspherical](http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/plot_kmeans-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means was trying to fit a square peg in a round hole- trying to find nice centers with neat spheres around them- and it failed.\n",
    "\n",
    "You might say “That’s not a fair example… no clustering method could correctly find clusters that are that weird.” Not true! Try single linkage hierachical clustering, which we will learn about later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unevenly sized clusters\n",
    "What if the clusters have an uneven number of points- does that also break k-means clustering? Well, consider this set of clusters, of sizes 20, 100, 500.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![unevenclusters](http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/different_sizes-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like k-means could probably find those clusters, right? Everything seems to be generated into neat and tidy groups. So let’s try k-means:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![clustering_unevens](http://varianceexplained.org/figs/2015-01-16-kmeans-free-lunch/different_sizes_kmeans-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem Scenario 3\n",
    "\n",
    "# Third toy data set\n",
    "blob1, y1 = datasets.make_blobs(n_samples=100,\n",
    "                               centers=[(10,8)],\n",
    "                               cluster_std=0.5,\n",
    "                               random_state=2)\n",
    "\n",
    "blob2, y2 = datasets.make_blobs(n_samples=600,\n",
    "                               centers=[(2,2)],\n",
    "                               cluster_std=2.5,\n",
    "                               random_state=2)\n",
    "\n",
    "\n",
    "unbal = np.vstack([blob1, blob2])\n",
    "y1[y1 == 0] = 0\n",
    "y2[y2 == 0] = 1\n",
    "labs = np.concatenate([y1, y2])\n",
    "\n",
    "#Predict K-Means cluster membership\n",
    "km_unbal = KMeans(n_clusters=2, random_state=2).fit(unbal)\n",
    "km_unbal_preds = KMeans(n_clusters=2, random_state=2).fit_predict(unbal)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(121, title='Problem Clusters Example 3')\n",
    "plt.scatter(unbal[:,0], unbal[:,1])\n",
    "plt.subplot(122, title='Problem Clusters Example 3 K-means labels')\n",
    "plt.scatter(unbal[:,0], unbal[:,1], c=km_unbal_preds, cmap=cmap)\n",
    "plt.scatter(km_unbal.cluster_centers_[:,0], km_unbal.cluster_centers_[:,1], marker='X', s=150, c='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "### K-means on larger dataset\n",
    "\n",
    "You want to run a wine subscription service, but you have no idea about wine tasting notes. You are a person of science.\n",
    "You have a wine dataset of scientific measurements.\n",
    "If you know a customer likes a certain wine in the dataset, can you recommend other wines to the customer in the same cluster?\n",
    "\n",
    "Questions:\n",
    "- How many clusters are in the wine dataset?\n",
    "- What are the characteristics of each clusters?\n",
    "- What problems do you see potentially in the data?\n",
    "\n",
    "the dataset is `Wine.csv`\n",
    "the link for the dataset can be found here: https://raw.githubusercontent.com/aapeebles/kmeans_flatiron/master/Kmeans%20lesson/Wine.csv\n",
    "\n",
    "\n",
    "Instructions:\n",
    "- First, remove customer_segment from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on problem here:\n",
    "url = 'https://raw.githubusercontent.com/aapeebles/kmeans_flatiron/master/Kmeans%20lesson/Wine.csv'\n",
    "wine = pd.read_csv(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  wine.drop(columns=['Customer_Segment'])\n",
    "segs = wine.Customer_Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to try and get an idea of how many different segments we might want, so we are going to start by plotting our data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select two features to create  a scatterplot of the wines \n",
    "\n",
    "plt.scatter(wine._____, wine.____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a K-means model and use a different evaluations of the clusters to decide the optimum number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the dataset and initializing variables\n",
    "X = wine\n",
    "distorsions = []\n",
    "\n",
    "# Calculate SSE for different K\n",
    "for k in range(__, __):\n",
    "    kmeans = KMeans(n_clusters=k, random_state = 10)\n",
    "    kmeans.fit(X)\n",
    "    distorsions.append(kmeans.inertia_)\n",
    "\n",
    "# Plot values of SSE\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(121, title='Elbow curve')\n",
    "plt.xlabel('k')\n",
    "plt.plot(range(_____, _____), distorsions)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate silhouette coefficient for each k\n",
    "X = features\n",
    "silhouette_plot = []\n",
    "for k in range(__, __):\n",
    "    clusters = KMeans(n_clusters=k, random_state=10)\n",
    "    cluster_labels = clusters.fit_predict(X)\n",
    "    silhouette_avg = metrics.silhouette_score(X, cluster_labels)\n",
    "    silhouette_plot.append(silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Silhouette coefficient\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(121, title='Silhouette coefficients over k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('silhouette coefficient')\n",
    "plt.plot(range(____, _____), silhouette_plot)\n",
    "plt.axhline(y=np.mean(silhouette_plot), color=\"red\", linestyle=\"--\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you think is the ideal number of clusters to use? How did you choose that number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the ideal number of clusters, let's refit our model and assign the oberservations to classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=_____, random_state = 10)\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(_____)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize those classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(wine.____, wine.____, c=____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-D graphing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Code source: Gaël Varoquaux\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Though the following import is not directly being used, it is required\n",
    "# for 3D projection to work\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "estimators = [('k_means_iris_8', KMeans(n_clusters=8)),\n",
    "              ('k_means_iris_3', KMeans(n_clusters=3)),\n",
    "              ('k_means_iris_bad_init', KMeans(n_clusters=3, n_init=1,\n",
    "                                               init='random'))]\n",
    "\n",
    "fignum = 1\n",
    "titles = ['8 clusters', '3 clusters', '3 clusters, bad initialization']\n",
    "for name, est in estimators:\n",
    "    fig = plt.figure(fignum, figsize=(4, 3))\n",
    "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "    est.fit(X)\n",
    "    labels = est.labels_\n",
    "\n",
    "    ax.scatter(X[:, 3], X[:, 0], X[:, 2],\n",
    "               c=labels.astype(np.float), edgecolor='k')\n",
    "\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "    ax.set_xlabel('Petal width')\n",
    "    ax.set_ylabel('Sepal length')\n",
    "    ax.set_zlabel('Petal length')\n",
    "    ax.set_title(titles[fignum - 1])\n",
    "    ax.dist = 12\n",
    "    fignum = fignum + 1\n",
    "\n",
    "# Plot the ground truth\n",
    "fig = plt.figure(fignum, figsize=(4, 3))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "\n",
    "for name, label in [('Setosa', 0),\n",
    "                    ('Versicolour', 1),\n",
    "                    ('Virginica', 2)]:\n",
    "    ax.text3D(X[y == label, 3].mean(),\n",
    "              X[y == label, 0].mean(),\n",
    "              X[y == label, 2].mean() + 2, name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor='k')\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Petal width')\n",
    "ax.set_ylabel('Sepal length')\n",
    "ax.set_zlabel('Petal length')\n",
    "ax.set_title('Ground Truth')\n",
    "ax.dist = 12\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
